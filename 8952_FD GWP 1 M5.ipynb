{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54cc3184-eb7d-40b8-8aff-30a63700e0c5",
   "metadata": {},
   "source": [
    "# GROUP WORK PROJECT #1  \n",
    "## MScFE 600: Financial Data  \n",
    "**Group Number:** 8952  \n",
    "\n",
    "---\n",
    "\n",
    "### Contributors\n",
    "\n",
    "| FULL LEGAL NAME                    | LOCATION (COUNTRY) | EMAIL ADDRESS                  |\n",
    "|------------------------------------|--------------------|--------------------------------|\n",
    "| Desire Bikorimana                 | Rwanda             | bikorimanadesire@yahoo.com     |\n",
    "| Cherukumilli Satya Sri Hari Charan| India              | haricharan.nitt@gmail.com      |\n",
    "| Bismark Mawuenyega Agbemafle      | Ghana              | agbemaflebismark@gmail.com     |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7cd05d",
   "metadata": {},
   "source": [
    "## Task 1: Data Quality\n",
    "\n",
    "### a. Example of Poor Quality Structured Data\n",
    "\n",
    "Poor quality structured data can take many forms. Consider a **CSV file of loan records** where interest rates are inconsistently formatted—some listed as decimals (e.g., `0.5`) and others as percentages (e.g., `5%`). Additionally, critical columns like `Loan_received_date` and `amount_received` may have missing or improperly formatted values. Similarly, a **sales database** may contain entries with invalid dates (e.g., `\"2023-02-30\"`), missing customer IDs, and duplicate transaction records (Kimball & Caserta, 2004).\n",
    "\n",
    "### b. Recognizing Poor Quality in Structured Data\n",
    "\n",
    "Data quality issues in such structured datasets can be identified using the following criteria:\n",
    "\n",
    "1. **Validity**: Values like `\"2023-02-30\"` are invalid since February has only 28 or 29 days (ISO 8000, 2023).\n",
    "2. **Completeness**: Missing key fields like customer IDs or loan amounts hinder analysis.\n",
    "3. **Consistency**: Mixing formats (fractions vs percentages) and duplicate records distort analytics and financial metrics.\n",
    "4. **Metadata Alignment**: Numeric fields may contain non-numeric values, violating expected data types (Consoli et al.).\n",
    "\n",
    "### c. Example of Poor Quality Unstructured Data\n",
    "\n",
    "An example of poor quality unstructured data is a **collection of financial news articles** scraped from various websites. These may be cluttered with HTML tags, navigation links, inconsistent formatting, typos, and even irrelevant advertisements. Alternatively, a **corpus of customer feedback emails** may include slang, spelling errors, and incomplete sentences (Batini et al., 2009).\n",
    "\n",
    "### d. Challenges in Assessing Unstructured Data\n",
    "\n",
    "Unstructured data often lacks key quality attributes:\n",
    "\n",
    "1. **Standardization**: Free-form text varies in format, making automated analysis challenging (IBM, 2021).\n",
    "2. **Accuracy**: Typos, slang (e.g., `\"It's gr8!\"`), and formatting issues lead to misinterpretation.\n",
    "3. **Relevance**: HTML artifacts and unrelated content introduce noise and dilute actionable insights.\n",
    "4. **Structure**: Lack of clear delimiters between useful and irrelevant content hampers natural language processing tasks (Consoli et al.).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23c1416",
   "metadata": {},
   "source": [
    "## Task 2: Yield Curve Modeling — Nelson-Siegel and Cubic Spline\n",
    "\n",
    "### Objective\n",
    "The objective of this task is to model the term structure of interest rates using two techniques: the Nelson-Siegel parametric model and a non-parametric cubic spline interpolation. These models will be applied to Indian government bond yields to analyze the shape of the yield curve and assess model accuracy.\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "To model the Indian yield curve, we utilize monthly yield data from the **Reserve Bank of India (RBI)** sourced from their [Government Securities Market database](https://data.rbi.org.in/DBIE/#/dbie/reports/Statistics/Financial%20Market/Government%20Securities%20Market).\n",
    "\n",
    "We use two primary datasets:\n",
    "\n",
    "1. **HBS Table No. 180** – *Yield of SGL Transactions in Government Dated Securities for Various Maturities*  \n",
    "   - Frequency: Monthly  \n",
    "   - Includes medium- and long-term yields ranging from 1 to 30 years  \n",
    "   - Source file: `HBS Table No. 180 _ Yield of SGL Transactions in Government Dated Securiites for Various Maturities.txt`\n",
    "\n",
    "This allows us to construct a cross-sectional yield curve with maturities spanning from **1 to 30 years** — ideal for fitting both the **Nelson-Siegel model** and **Cubic Spline interpolation**.\n",
    "\n",
    "We select a **single representative month** from the latest available data to focus on the shape of the yield curve at that point in time, aligning with the project’s goal of static curve modeling.\n",
    "\n",
    "### Data Export and Manual Formatting\n",
    "\n",
    "The monthly yield data for government dated securities for the financial year **2024–25** was extracted from the RBI’s HBS Table No. 180. We limited the data to **April 2024 to March 2025**, covering maturities from **1 year to 30 years**.\n",
    "\n",
    "The extracted data was exported to a CSV file (`yield_curve_2024_25.csv`) and then manually formatted to relabel the column headers to reflect full month-year labels (e.g., `\"Apr 2024\"`, `\"May 2024\"`, ..., `\"Mar 2025\"`). The final exported file is named:\n",
    "\n",
    "> `yield_curve_2024_25.csv`\n",
    "\n",
    "No additional cleaning steps such as `dropna()` or imputation were necessary, as the dataset did not contain missing values in the selected block.\n",
    "\n",
    "This clean, structured dataset is now ready for use in **Nelson-Siegel** and **Cubic Spline** yield curve fitting.\n",
    "\n",
    "### Modeling Approach\n",
    "\n",
    "**Nelson-Siegel Model**: Fit a parametric model using the functional form:\n",
    "\n",
    "$$\n",
    "y(\\tau) = \\beta_0 + \\beta_1 \\frac{1 - e^{-\\lambda \\tau}}{\\lambda \\tau} + \\beta_2 \\left( \\frac{1 - e^{-\\lambda \\tau}}{\\lambda \\tau} - e^{-\\lambda \\tau} \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\tau = \\text{time to maturity in years}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta_0 = \\text{Long-term yield (level)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta_1 = \\text{Short-term component (slope)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta_2 = \\text{Medium-term curvature}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda = \\text{Decay factor controlling the exponential term}\n",
    "$$\n",
    "\n",
    "**Cubic Spline Interpolation**: Fit a smooth curve through the yield data using cubic spline interpolation via `scipy.interpolate.make_interp_spline`.\n",
    "\n",
    "### Evaluation and Interpretation\n",
    "- Compare the two models using fit metrics such as RMSE or R².\n",
    "- Plot yield curves to visualize differences in smoothness and fit.\n",
    "- Discuss the interpretability of parameters in each model and their economic implications.\n",
    "\n",
    "### Ethical Consideration\n",
    "We reflect on whether yield curve smoothing introduces bias or obscures important information. The ethical implications of using smoothed models in financial reporting and risk management will be considered.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
